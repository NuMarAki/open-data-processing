#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Menu interativo principal - Sistema refatorado mantendo a interface original"""

import os
import sys
from datetime import datetime
import shutil
import pandas as pd
import pyarrow.parquet as pq
import glob
from typing import Optional, List, Dict
import config_manager
from utils import CBOS_TI, criar_caminho, logger
import matplotlib.pyplot as plt
import gc
import configparser
import unicodedata
import re
from processadores_especificos import ProcessadorPNAD, ProcessadorRAIS, ProcessadorCAGED
from processador_base import ProcessadorSeguro
from analise_etarismo import AnalisadorEtarismo
from descompactador import Descompactador
from utils import (
    ler_arquivo_com_fallback, logger, configurar_log, monitorar_recursos, verificar_espaco_disco,
    limpar_diretorio, arquivo_existe_e_tamanho_ok, limpar_temp_personalizado, listar_erros_descompactacao,
    criar_caminho, CBOS_TI
)

diretorio_temp = r'Z:\TCC\TEMP'

try:
    os.makedirs(diretorio_temp, exist_ok=True)
    print(f"Diret√≥rio '{diretorio_temp}' garantido.")
except OSError as e:
    print(f"Erro ao criar o diret√≥rio '{diretorio_temp}': {e}")

os.environ['TEMP'] = diretorio_temp
os.environ['TMP'] = diretorio_temp

def _processar_chunk_para_metricas(chunk: pd.DataFrame, colunas_essenciais: List[str]) -> pd.DataFrame:
    """Normaliza um chunk e cria eh_ti se necess√°rio."""
    # Adiciona colunas faltantes em lote (evita fragmenta√ß√£o)
    faltantes = [c for c in colunas_essenciais if c not in chunk.columns]
    if faltantes:
        chunk[faltantes] = None

    # Cria eh_ti se faltar, a partir de cbo_ocupacao
    if 'eh_ti' not in chunk.columns:
        if 'cbo_ocupacao' in chunk.columns:
            cbo = pd.to_numeric(chunk['cbo_ocupacao'], errors='coerce').astype('Int64').astype(str)
            chunk['eh_ti'] = cbo.isin(CBOS_TI)
        else:
            chunk['eh_ti'] = False

    return chunk

def _iterar_parquet_em_batches(arquivo: str, colunas: List[str], batch_size: int = 200_000):
    """Itera Parquet em batches usando PyArrow para reduzir pico de mem√≥ria."""
    pf = pq.ParquetFile(arquivo)
    cols = [c for c in colunas if c in pf.schema.names]
    for batch in pf.iter_batches(columns=cols or None, batch_size=batch_size):
        yield batch.to_pandas(types_mapper=pd.ArrowDtype, self_destruct=True)

def verificar_arquivo_consolidado_existe(tipo):
    """Verifica se j√° existe arquivo consolidado para a base"""
    from utils import criar_caminho
    
    arquivo_consolidado = criar_caminho('C:/TCC/dados', tipo, f'dados_{tipo}_consolidados.csv')
    arquivo_parquet = criar_caminho('C:/TCC/dados', tipo, f'dados_{tipo}_consolidados.parquet')
    
    # Verificar se existe arquivo consolidado (CSV ou Parquet)
    if os.path.exists(arquivo_consolidado):
        tamanho_mb = os.path.getsize(arquivo_consolidado) / (1024*1024)
        if tamanho_mb > 1:  # Arquivo deve ter pelo menos 1MB
            return arquivo_consolidado, tamanho_mb
    
    if os.path.exists(arquivo_parquet):
        tamanho_mb = os.path.getsize(arquivo_parquet) / (1024*1024)
        if tamanho_mb > 1:  # Arquivo deve ter pelo menos 1MB
            return arquivo_parquet, tamanho_mb
    
    return None, 0

def verificar_configuracoes():
    """Verifica configura√ß√µes e espa√ßo, al√©m de identificar duplicatas nos arquivos .cfg"""
    configs = ['colunas_pnad.cfg', 'colunas_rais.cfg', 'colunas_caged.cfg']
    config_ok = True
    
    for config_file in configs:
        if not os.path.exists(config_file):
            logger.error(f"Arquivo de configura√ß√£o n√£o encontrado: {config_file}")
            config_ok = False
            continue

        # Ler o arquivo .cfg com configparser
        try:
            parser = configparser.ConfigParser()
            parser.read(config_file, encoding='utf-8')
            
            # Verificar duplicatas dentro de cada se√ß√£o
            for section in parser.sections():
                colunas = parser.options(section)
                vistos = set()
                duplicados = set()
                for col in colunas:
                    if col in vistos:
                        duplicados.add(col)
                    vistos.add(col)
                
                if duplicados:
                    logger.error(f"ERRO: Nomes de colunas duplicados encontrados na se√ß√£o '{section}' em '{config_file}': {list(duplicados)}")
                    print(f"‚ùå ERRO: Nomes de colunas duplicados encontrados na se√ß√£o '{section}' em '{config_file}': {list(duplicados)}")
                    config_ok = False
        except Exception as e:
            logger.error(f"Erro ao ler o arquivo de configura√ß√£o '{config_file}': {e}")
            print(f"‚ùå Erro ao ler o arquivo de configura√ß√£o '{config_file}': {e}")
            config_ok = False

    if not config_ok:
        return False  # Retorna imediatamente se houver erro de configura√ß√£o

    # Verificar espa√ßo
    tem_espaco, espaco_gb = verificar_espaco_disco()
    print(f"\nüíæ Espa√ßo dispon√≠vel: {espaco_gb:.1f} GB")
    
    if espaco_gb < 5:
        print("‚ö†Ô∏è  AVISO: Pouco espa√ßo em disco!")
        print("   Considere limpar arquivos tempor√°rios ou descompactados.")
    
    return True

def ajustar_colunas_ocupacao(df):
    """
    Renomeia a primeira coluna que corresponda a varia√ß√µes de 'CBO' para 'cbo_ocupacao'.
    Normaliza acentos, remove caracteres estranhos e aplica .strip().
    """
    for col in list(df.columns):
        col_norm = unicodedata.normalize('NFKD', col).encode('ASCII', 'ignore').decode('ASCII').lower().strip()
        # remover caracteres n√£o alfanum√©ricos exceto underscore e espa√ßo
        col_norm = re.sub(r'[^a-z0-9_ ]', '', col_norm)
        if col_norm.startswith('cbo') or re.match(r'^cbo[_\s]', col_norm):
            if 'cbo_ocupacao' not in df.columns:
                df.rename(columns={col: 'cbo_ocupacao'}, inplace=True)
            break
    return df

def menu_principal():
    print("\n" + "="*60)
    print("AN√ÅLISE DE ETARISMO EM TI NO BRASIL - v2.0")
    print("TCC - MBA Data Science USP/Esalq")
    print("="*60)
    print("\nMENU PRINCIPAL")
    print("1. Descompacta√ß√£o de Arquivos")
    print("2. Processar Bases")
    print("3. Relat√≥rios e An√°lises")
    print("4. Gerar diagn√≥stico dos dados")  # <-- Nova op√ß√£o
    print("0. Sair")
    return input("\nEscolha uma op√ß√£o: ").strip()


def submenu_processar_bases():
    print("\n--- Processar Bases ---")
    print("1. Processar PNAD")
    print("2. Processar RAIS")
    print("3. Processar CAGED")
    print("4. Processar TODAS")
    print("0. Voltar")
    return input("Escolha uma op√ß√£o: ").strip()


def submenu_relatorios():
    print("\n--- Relat√≥rios e An√°lises ---")
    print("1. Relat√≥rio PNAD")
    print("2. Relat√≥rio RAIS")
    print("3. Relat√≥rio CAGED")
    print("4. Relat√≥rio CONSOLIDADO (TODAS)")
    print("0. Voltar")
    return input("Escolha uma op√ß√£o: ").strip()


def submenu_descompactacao():
    print("\n--- Descompacta√ß√£o de Arquivos ---")
    print("1. Descompactar PNAD")
    print("2. Descompactar RAIS")
    print("3. Descompactar CAGED")
    print("4. Descompactar TODAS")
    print("5. Listar arquivos com erro de descompacta√ß√£o")  # <-- Adicionada esta linha
    print("0. Voltar")
    return input("Escolha uma op√ß√£o: ").strip()


def encontrar_partes_consolidado(arquivo_base):
    """Encontra todas as partes de um arquivo consolidado"""
    partes = []
    
    # Verificar arquivo base
    if os.path.exists(arquivo_base):
        partes.append(arquivo_base)
    
    # Procurar por partes parquet
    dir_base = os.path.dirname(arquivo_base)
    nome_base = os.path.basename(arquivo_base).replace('.csv', '')
    
    # Procurar parquet correspondente
    arquivo_parquet = arquivo_base.replace('.csv', '.parquet')
    if os.path.exists(arquivo_parquet):
        partes = [arquivo_parquet]  # Preferir parquet
        
    # Procurar por m√∫ltiplas partes
    for ext in ['.parquet', '.csv', '.csv.gz']:
        pattern = os.path.join(dir_base, f"{nome_base}_parte_*{ext}")
        partes.extend(glob.glob(pattern))
    
    return sorted(list(set(partes)))  # Remover duplicatas


def consolidar_base(tipo):
    """Consolida dados preprocessados de uma base de forma incremental, sem filtrar TI, para todos os tipos"""
    from utils import criar_caminho
    import pyarrow.parquet as pq
    import gc

    dir_preprocessados = criar_caminho('C:/TCC/dados', tipo, 'preprocessados')
    arquivo_saida = criar_caminho('C:/TCC/dados', tipo, f'dados_{tipo}_consolidados.csv')
    arquivo_cfg = f'colunas_{tipo}.cfg'  # Adicionado para carregar colunas

    if tipo not in ['pnad', 'rais', 'caged']:
        print(f"Tipo de base desconhecido: {tipo}")
        return

    arquivos = glob.glob(os.path.join(dir_preprocessados, '*.csv')) + \
               glob.glob(os.path.join(dir_preprocessados, '*.parquet'))

    if not arquivos:
        print(f"Nenhum arquivo preprocessado encontrado para {tipo.upper()} em {dir_preprocessados}")
        return

    print(f"\n[{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}] INICIANDO CONSOLIDA√á√ÉO DE {tipo.upper()}...")
    print(f"{tipo.upper()}: {len(arquivos)} arquivos encontrados para consolida√ß√£o:")

    for arq in arquivos:
        print(f" - {os.path.basename(arq)}")

    # Carregar nomes de colunas do CFG como garantia
    colunas_cfg = None
    if os.path.exists(arquivo_cfg):
        with open(arquivo_cfg, encoding='utf-8') as f:
            colunas_cfg = [line.strip() for line in f if line.strip()]
            print(f"   ‚úÖ Nomes de colunas carregados de {arquivo_cfg}")

    # Remover arquivo antigo, se existir
    if os.path.exists(arquivo_saida):
        os.remove(arquivo_saida)

    total = len(arquivos)
    colunas_padrao = colunas_cfg  # Usar colunas do CFG como padr√£o
    total_registros = 0
    primeiro = True
    chunk_size = 500_000

    print("\nIniciando consolida√ß√£o incremental...")
    for i, arq in enumerate(arquivos, 1):
        try:
            if arq.endswith('.parquet'):
                # Parquet j√° tem colunas, a leitura √© direta
                chunk = pd.read_parquet(arq, engine='pyarrow')
                # --- AJUSTE DE COLUNA DE OCUPA√á√ÉO ---
                chunk = ajustar_colunas_ocupacao(chunk)
                if colunas_padrao is None:
                    colunas_padrao = list(chunk.columns)
                # Alinhar colunas e garantir que colunas essenciais estejam presentes
                    colunas_essenciais = ['idade', 'cbo_ocupacao']
                    for col in colunas_padrao + colunas_essenciais:
                        if col not in chunk.columns:
                            chunk[col] = None  # Adicionar colunas ausentes
                chunk = chunk[colunas_padrao]
                # Criar a coluna 'eh_ti' com base em crit√©rios (exemplo: ocupa√ß√µes relacionadas √† TI)
                if 'eh_ti' not in chunk.columns:
                    chunk['eh_ti'] = chunk['cbo_ocupacao'].apply(lambda x: x in ['1234', '5678'] if pd.notnull(x) else False)

                chunk.to_csv(arquivo_saida, sep=';', index=False, header=primeiro, mode='a', encoding='utf-8')
                primeiro = False
                total_registros += len(chunk)
                del chunk
                gc.collect()
            else: # Para arquivos CSV
                # For√ßar o uso das colunas do CFG
                for chunk in ler_arquivo_com_fallback(arq, sep=';', chunk_size=chunk_size, colunas=colunas_padrao):
                    # --- AJUSTE DE COLUNA DE OCUPA√á√ÉO ---
                    chunk = ajustar_colunas_ocupacao(chunk)

                    # Verificar e corrigir colunas duplicadas
                    if chunk.columns.duplicated().any():
                        logger.warning(f"Colunas duplicadas detectadas em {arq}. Corrigindo...")
                        chunk = corrigir_colunas_duplicadas(chunk)

                    # Adicionar colunas ausentes de uma vez
                    colunas_ausentes = [col for col in colunas_padrao if col not in chunk.columns]
                    if colunas_ausentes:
                        chunk = pd.concat([chunk, pd.DataFrame(columns=colunas_ausentes)], axis=1)

                    # Reordenar as colunas
                    chunk = chunk[colunas_padrao]

                    # Alinhar colunas e garantir que colunas essenciais estejam presentes
                    colunas_essenciais = ['idade', 'cbo_ocupacao']
                    for col in colunas_padrao + colunas_essenciais:
                        if col not in chunk.columns:
                            chunk[col] = None  # Adicionar colunas ausentes

                    # Criar a coluna 'eh_ti' com base em crit√©rios (exemplo: ocupa√ß√µes relacionadas √† TI)
                    if 'eh_ti' not in chunk.columns:
                        chunk['eh_ti'] = chunk['cbo_ocupacao'].apply(lambda x: x in ['1234', '5678'] if pd.notnull(x) else False)

                    # Salvar o chunk no arquivo consolidado
                    chunk.to_csv(arquivo_saida, sep=';', index=False, header=primeiro, mode='a', encoding='utf-8')
                    primeiro = False
                    total_registros += len(chunk)
                    del chunk
                    gc.collect()
            porcentagem = (i / total) * 100
            print(f"[{i}/{total}] {os.path.basename(arq)} - {total_registros:,} registros acumulados ({porcentagem:.1f}% conclu√≠do)")
        except Exception as e:
            print(f"Erro ao processar {arq}: {e}")
    print(f"\n‚úÖ Consolida√ß√£o incremental conclu√≠da!")
    print(f"   Total de registros: {total_registros:,}")
    print(f"   Arquivo salvo: {arquivo_saida}")


def contar_linhas_fallback_encoding(arquivo, encodings=['utf-8', 'latin1', 'utf-8']):
    for enc in encodings:
        try:
            with open(arquivo, 'r', encoding=enc) as f:
                total = sum(1 for _ in f) - 1
                print(f"[INFO] Contagem de linhas em {arquivo} com encoding: {enc}")
                return total
        except Exception as e:
            print(f"[WARN] Falha ao contar linhas em {arquivo} com encoding {enc}: {e}")
    print(f"[ERRO] N√£o foi poss√≠vel contar linhas em {arquivo} com nenhum encoding conhecido.")
    return None


# Fun√ß√£o para carregar colunas de um arquivo .cfg respeitando as se√ß√µes
def carregar_colunas_cfg(arquivo_cfg):
    """Carrega as colunas de um arquivo .cfg, organizadas por se√ß√£o."""
    parser = configparser.ConfigParser()
    parser.read(arquivo_cfg, encoding='utf-8')
    
    colunas_por_secao = {}
    for section in parser.sections():
        colunas_por_secao[section] = parser.options(section)
    return colunas_por_secao

# Ajuste na fun√ß√£o processar_base_completa
def processar_base_completa(nome_base, ProcessadorClasse, arquivo_cfg):
    """Processa uma base de dados completa"""
    from utils import criar_caminho
    
    print(f"\n{'='*60}")
    print(f"PROCESSANDO {nome_base}")
    print(f"{'='*60}")
    
    # PRIMEIRA VERIFICA√á√ÉO: Arquivo consolidado j√° existe?
    arquivo_existente, tamanho_mb = verificar_arquivo_consolidado_existe(nome_base.lower())
    
    if arquivo_existente:
        print(f"\n‚úÖ {nome_base} j√° possui arquivo consolidado:")
        print(f"   ‚Ä¢ {os.path.basename(arquivo_existente)} ({tamanho_mb:.1f} MB)")
        
        # Perguntar se quer reprocessar
        resposta = input(f"\nDeseja reprocessar {nome_base} do zero? (S/N): ").strip().upper()
        if resposta != 'S':
            print(f"üìä Executando an√°lise com dados existentes...")
            
            # Ir direto para an√°lise
            dir_resultado = criar_caminho('resultados', nome_base.lower())
            os.makedirs(dir_resultado, exist_ok=True)
            
            analisador = AnalisadorEtarismo(dir_resultado)
            analisador.executar_analise_completa(arquivo_existente)
            
            print(f"‚úÖ An√°lise de {nome_base} conclu√≠da!")
            return True
        else:
            print(f"üîÑ Reprocessando {nome_base} do zero...")
            # Remover arquivo consolidado existente para for√ßar reprocessamento
            try:
                if os.path.exists(arquivo_existente):
                    os.remove(arquivo_existente)
                    print(f"   Arquivo consolidado removido: {os.path.basename(arquivo_existente)}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erro ao remover arquivo existente: {e}")
    
    # SEGUNDA VERIFICA√á√ÉO: Verificar se h√° partes consolidadas (sistema antigo)
    arquivo_consolidado = criar_caminho('dados', nome_base.lower(), f'dados_{nome_base.lower()}_consolidados.csv')
    partes = encontrar_partes_consolidado(arquivo_consolidado)
    
    if partes and not arquivo_existente:  # Se n√£o foi detectado na primeira verifica√ß√£o
        print(f"\n‚úÖ {nome_base} possui dados em partes:")
        for parte in partes:
            tamanho_mb = os.path.getsize(parte) / (1024*1024)
            print(f"   ‚Ä¢ {os.path.basename(parte)} ({tamanho_mb:.1f} MB)")
            
        resposta = input(f"\nDeseja reprocessar {nome_base}? (S/N): ").strip().upper()
        if resposta != 'S':
            print(f"üìä Executando an√°lise com partes existentes...")
            
            dir_resultado = criar_caminho('resultados', nome_base.lower())
            os.makedirs(dir_resultado, exist_ok=True)
            
            for idx, parte in enumerate(partes, 1):
                if os.path.exists(parte):
                    print(f"\n‚û°Ô∏è  Processando arquivo {idx}/{len(partes)}: {os.path.basename(parte)}")
                    analisador = AnalisadorEtarismo(dir_resultado)
                    analisador.executar_analise_completa(parte)
                    
            return True
        else:
            # Limpar partes existentes
            for parte in partes:
                try:
                    if os.path.exists(parte):
                        os.remove(parte)
                        print(f"   Parte removida: {os.path.basename(parte)}")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Erro ao remover {parte}: {e}")
    
    # TERCEIRA PARTE: Processamento completo (descompacta√ß√£o + preprocessamento + consolida√ß√£o)
    print(f"\nüîÑ Iniciando processamento completo de {nome_base}...")
    
    # Verificar espa√ßo antes
    tem_espaco, espaco_gb = verificar_espaco_disco()
    print(f"   Espa√ßo dispon√≠vel: {espaco_gb:.1f} GB")
    
    if espaco_gb < 5:
        print("   ‚ö†Ô∏è Limpando tempor√°rios...")
        limpar_temporarios(criar_caminho('dados', nome_base.lower(), 'temp'))
    
    print(f"   üîÑ Reprocessamento autom√°tico de falhas ativado para {nome_base}")
    
    try:
        # Carregar configura√ß√£o
        config = config_manager.ConfigManager().carregar_configuracao(nome_base.lower(), arquivo_cfg)
        
        # For√ßar processamento sequencial se usar_paralelo=False
        if not config.usar_paralelo:
            print(f"üìù Processamento sequencial ativado (usar_paralelo=False)")
            config.usar_paralelo = False
            config.max_workers = 1
        
        # Criar processador
        processador = ProcessadorClasse(config)
        
        # Adicionar o caminho do CFG ao processador para que ele possa us√°-lo
        processador.arquivo_cfg = arquivo_cfg

        # Processar com seguran√ßa
        with ProcessadorSeguro(processador) as proc:
            df = proc.processar_periodo_completo()
        
        if df is not None:
            print(f"‚úÖ {nome_base} processada com sucesso!")
            print(f"   Total de registros: {len(df):,}")
            
            # Usar o caminho onde o processador realmente salvou
            arquivo_consolidado_real = criar_caminho('C:/TCC/dados', nome_base.lower(), f'dados_{nome_base.lower()}_consolidados.csv')

            # Salvar an√°lises em resultados
            dir_resultado = criar_caminho('resultados', nome_base.lower())
            os.makedirs(dir_resultado, exist_ok=True)

            # Usar o arquivo real para an√°lise
            arquivo_saida = arquivo_consolidado_real
            
            print(f"üìä Executando an√°lise de etarismo...")
            analisador = AnalisadorEtarismo(dir_resultado)
            analisador.executar_analise_completa(arquivo_saida)
            
            return True
        else:
            print(f"‚ùå Falha no processamento de {nome_base}")
            return False
            
    except Exception as e:
        print(f"‚ùå Erro: {e}")
        logger.error(f"Erro ao processar {nome_base}: {e}")
        return False

def verificar_arquivos_descompactados(tipo):
    from utils import criar_caminho
    dir_descompactados = criar_caminho('dados', tipo, 'descompactados')
    if not os.path.exists(dir_descompactados):
        return False
    arquivos = [f for f in os.listdir(dir_descompactados) if f.endswith('.txt')]
    return len(arquivos) > 0

def verificar_arquivos_preprocessados(tipo):
    from utils import criar_caminho
    dir_preprocessados = criar_caminho('dados', tipo, 'preprocessados')
    if not os.path.exists(dir_preprocessados):
        return False
    arquivos = glob.glob(os.path.join(dir_preprocessados, '*.csv')) + \
               glob.glob(os.path.join(dir_preprocessados, '*.parquet'))
    return len(arquivos) > 0

def descompactar_base(tipo, ProcessadorClasse, arquivo_cfg):
    print(f"\nDescompactando {tipo.upper()}...")
    try:
        config = config_manager.ConfigManager().carregar_configuracao(tipo, arquivo_cfg)
        
        # For√ßar descompacta√ß√£o sequencial se usar_paralelo=False
        if not config.usar_paralelo:
            print(f"üìù Descompacta√ß√£o sequencial ativada (usar_paralelo=False)")
            config.usar_paralelo = False
            config.max_workers = 1
        
        processador = ProcessadorClasse(config)
        descompactador = Descompactador(tipo, config.caminho_destino)
        arquivos = processador.descobrir_arquivos_compactados()
        print(f"Encontrados {len(arquivos)} arquivos para descompactar")
        sucesso = 0
        
        # Descompacta√ß√£o sequencial
        for i, arquivo in enumerate(arquivos, 1):
            print(f"üìÅ Descompactando arquivo {i}/{len(arquivos)}: {os.path.basename(arquivo)}")
            if descompactador.descompactar_arquivo(arquivo):
                sucesso += 1
                
        print(f"‚úÖ {tipo.upper()}: {sucesso}/{len(arquivos)} arquivos descompactados")
    except Exception as e:
        print(f"‚ùå Erro ao descompactar {tipo.upper()}: {e}")
        logger.error(f"Erro ao descompactar {tipo.upper()}: {e}")

def processar_base_se_preciso(tipo, ProcessadorClasse, arquivo_cfg):
    """Processa base apenas se necess√°rio, verificando primeiro se j√° existe consolidado"""
    
    # Verificar se j√° existe arquivo consolidado
    arquivo_existente, tamanho_mb = verificar_arquivo_consolidado_existe(tipo)
    
    if arquivo_existente:
        print(f"\n‚úÖ {tipo.upper()} j√° possui arquivo consolidado ({tamanho_mb:.1f} MB)")
        print(f"   Pulando descompacta√ß√£o e preprocessamento...")
        
        # Ir direto para an√°lise
        from utils import criar_caminho
        dir_resultado = criar_caminho('resultados', tipo)
        os.makedirs(dir_resultado, exist_ok=True)
        
        print(f"üìä Executando an√°lise de etarismo para {tipo.upper()}...")
        analisador = AnalisadorEtarismo(dir_resultado)
        analisador.executar_analise_completa(arquivo_existente)
        
        return True
    
    # Se n√£o existe consolidado, verificar se precisa descompactar
    if not verificar_arquivos_descompactados(tipo):
        print(f"‚ö†Ô∏è Arquivos descompactados de {tipo.upper()} n√£o encontrados. Iniciando descompacta√ß√£o...")
        descompactar_base(tipo, ProcessadorClasse, arquivo_cfg)
    
    # Processar base completa
    return processar_base_completa(tipo.upper(), ProcessadorClasse, arquivo_cfg)

def relatorio_base_se_preciso(tipo, ProcessadorClasse, arquivo_cfg, func_relatorio):
    if not verificar_arquivos_preprocessados(tipo):
        print(f"‚ö†Ô∏è Arquivos preprocessados de {tipo.upper()} n√£o encontrados. Iniciando processamento de base...")
        processar_base_se_preciso(tipo, ProcessadorClasse, arquivo_cfg)
    func_relatorio()

def salvar_csv_com_extensao_correta(df, caminho_arquivo, compressao=False, **kwargs):
    """Salva DataFrame como CSV, garantindo extens√£o .csv.gz se compress√£o gzip for usada."""
    if compressao:
        if not caminho_arquivo.endswith('.csv.gz'):
            caminho_arquivo = caminho_arquivo.replace('.csv', '') + '.csv.gz'
        df.to_csv(caminho_arquivo, sep=';', index=False, encoding='utf-8', compression='gzip', **kwargs)
    else:
        if not caminho_arquivo.endswith('.csv'):
            caminho_arquivo += '.csv'
        df.to_csv(caminho_arquivo, sep=';', index=False, encoding='utf-8', **kwargs)
    return caminho_arquivo


def processar_base_completa(nome_base, ProcessadorClasse, arquivo_cfg):
    """Processa uma base de dados completa"""
    from utils import criar_caminho
    
    print(f"\n{'='*60}")
    print(f"PROCESSANDO {nome_base}")
    print(f"{'='*60}")
    
    # PRIMEIRA VERIFICA√á√ÉO: Arquivo consolidado j√° existe?
    arquivo_existente, tamanho_mb = verificar_arquivo_consolidado_existe(nome_base.lower())
    
    if arquivo_existente:
        print(f"\n‚úÖ {nome_base} j√° possui arquivo consolidado:")
        print(f"   ‚Ä¢ {os.path.basename(arquivo_existente)} ({tamanho_mb:.1f} MB)")
        
        # Perguntar se quer reprocessar
        resposta = input(f"\nDeseja reprocessar {nome_base} do zero? (S/N): ").strip().upper()
        if resposta != 'S':
            print(f"üìä Executando an√°lise com dados existentes...")
            
            # Ir direto para an√°lise
            dir_resultado = criar_caminho('resultados', nome_base.lower())
            os.makedirs(dir_resultado, exist_ok=True)
            
            analisador = AnalisadorEtarismo(dir_resultado)
            analisador.executar_analise_completa(arquivo_existente)
            
            print(f"‚úÖ An√°lise de {nome_base} conclu√≠da!")
            return True
        else:
            print(f"üîÑ Reprocessando {nome_base} do zero...")
            # Remover arquivo consolidado existente para for√ßar reprocessamento
            try:
                if os.path.exists(arquivo_existente):
                    os.remove(arquivo_existente)
                    print(f"   Arquivo consolidado removido: {os.path.basename(arquivo_existente)}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erro ao remover arquivo existente: {e}")
    
    # SEGUNDA VERIFICA√á√ÉO: Verificar se h√° partes consolidadas (sistema antigo)
    arquivo_consolidado = criar_caminho('dados', nome_base.lower(), f'dados_{nome_base.lower()}_consolidados.csv')
    partes = encontrar_partes_consolidado(arquivo_consolidado)
    
    if partes and not arquivo_existente:  # Se n√£o foi detectado na primeira verifica√ß√£o
        print(f"\n‚úÖ {nome_base} possui dados em partes:")
        for parte in partes:
            tamanho_mb = os.path.getsize(parte) / (1024*1024)
            print(f"   ‚Ä¢ {os.path.basename(parte)} ({tamanho_mb:.1f} MB)")
            
        resposta = input(f"\nDeseja reprocessar {nome_base}? (S/N): ").strip().upper()
        if resposta != 'S':
            print(f"üìä Executando an√°lise com partes existentes...")
            
            dir_resultado = criar_caminho('resultados', nome_base.lower())
            os.makedirs(dir_resultado, exist_ok=True)
            
            for idx, parte in enumerate(partes, 1):
                if os.path.exists(parte):
                    print(f"\n‚û°Ô∏è  Processando arquivo {idx}/{len(partes)}: {os.path.basename(parte)}")
                    analisador = AnalisadorEtarismo(dir_resultado)
                    analisador.executar_analise_completa(parte)
                    
            return True
        else:
            # Limpar partes existentes
            for parte in partes:
                try:
                    if os.path.exists(parte):
                        os.remove(parte)
                        print(f"   Parte removida: {os.path.basename(parte)}")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Erro ao remover {parte}: {e}")
    
    # TERCEIRA PARTE: Processamento completo (descompacta√ß√£o + preprocessamento + consolida√ß√£o)
    print(f"\nüîÑ Iniciando processamento completo de {nome_base}...")
    
    # Verificar espa√ßo antes
    tem_espaco, espaco_gb = verificar_espaco_disco()
    print(f"   Espa√ßo dispon√≠vel: {espaco_gb:.1f} GB")
    
    if espaco_gb < 5:
        print("   ‚ö†Ô∏è Limpando tempor√°rios...")
        limpar_temporarios(criar_caminho('dados', nome_base.lower(), 'temp'))
    
    print(f"   üîÑ Reprocessamento autom√°tico de falhas ativado para {nome_base}")
    
    try:
        # Carregar configura√ß√£o
        config = config_manager.ConfigManager().carregar_configuracao(nome_base.lower(), arquivo_cfg)
        
        # For√ßar processamento sequencial se usar_paralelo=False
        if not config.usar_paralelo:
            print(f"üìù Processamento sequencial ativado (usar_paralelo=False)")
            config.usar_paralelo = False
            config.max_workers = 1
        
        # Criar processador
        processador = ProcessadorClasse(config)
        
        # Adicionar o caminho do CFG ao processador para que ele possa us√°-lo
        processador.arquivo_cfg = arquivo_cfg

        # Processar com seguran√ßa
        with ProcessadorSeguro(processador) as proc:
            df = proc.processar_periodo_completo()
        
        if df is not None:
            print(f"‚úÖ {nome_base} processada com sucesso!")
            print(f"   Total de registros: {len(df):,}")
            
            # Usar o caminho onde o processador realmente salvou
            arquivo_consolidado_real = criar_caminho('C:/TCC/dados', nome_base.lower(), f'dados_{nome_base.lower()}_consolidados.csv')

            # Salvar an√°lises em resultados
            dir_resultado = criar_caminho('resultados', nome_base.lower())
            os.makedirs(dir_resultado, exist_ok=True)

            # Usar o arquivo real para an√°lise
            arquivo_saida = arquivo_consolidado_real
            
            print(f"üìä Executando an√°lise de etarismo...")
            analisador = AnalisadorEtarismo(dir_resultado)
            analisador.executar_analise_completa(arquivo_saida)
            
            return True
        else:
            print(f"‚ùå Falha no processamento de {nome_base}")
            return False
            
    except Exception as e:
        print(f"‚ùå Erro: {e}")
        logger.error(f"Erro ao processar {nome_base}: {e}")
        return False


def limpar_temporarios(diretorio_base='C:/TCC/dados'):
    """Limpa arquivos tempor√°rios"""
    # Importa√ß√£o expl√≠cita para garantir disponibilidade
    from utils import criar_caminho
    
    diretorios_temp = [
        criar_caminho(diretorio_base, 'rais', 'temp'),
        criar_caminho(diretorio_base, 'caged', 'temp'), 
        criar_caminho(diretorio_base, 'pnad', 'temp'),
        'temp'
    ]
    
    for diretorio in diretorios_temp:
        if os.path.exists(diretorio):
            try:
                limpar_diretorio(diretorio, manter_diretorio=True)
                print(f"‚úÖ Limpo: {diretorio}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Erro ao limpar {diretorio}: {e}")


def executar_analise_consolidada(bases: Optional[List[str]] = None):
    """Executa a an√°lise consolidada apenas nas bases informadas (['pnad','rais','caged'])."""
    # Inicializa resultados para evitar NameError
    resultados: Dict[str, dict] = {}

    validas = {'pnad', 'rais', 'caged'}
    if bases is None:
        selecionadas = ['pnad', 'rais', 'caged']
    else:
        if isinstance(bases, str):
            bases = [bases]
        selecionadas = [b.lower() for b in bases if b.lower() in validas]

    logger.info(f"Bases selecionadas para an√°lise: {selecionadas if selecionadas else 'nenhuma'}")
    if not selecionadas:
        logger.warning("Nenhuma base v√°lida selecionada. Abortando an√°lise consolidada.")
        return resultados

    bases_dados = {
        'pnad': {'dir': criar_caminho('C:/TCC/dados', 'pnad', 'preprocessados')},
        'rais': {'dir': criar_caminho('C:/TCC/dados', 'rais', 'preprocessados')},
        'caged': {'dir': criar_caminho('C:/TCC/dados', 'caged', 'preprocessados')},
    }

    def _ler_csv_em_chunks(caminho: str, cols_desejadas: List[str], chunksize=500_000):
        # L√™ cabe√ßalho para montar usecols seguro
        try:
            header_cols = list(pd.read_csv(caminho, sep=';', nrows=0).columns)
            usecols = [c for c in cols_desejadas if c in header_cols]
        except Exception:
            usecols = None
        for chunk in pd.read_csv(caminho, sep=';', usecols=usecols, chunksize=chunksize, low_memory=False):
            yield chunk

    for base in selecionadas:
        dir_pre = bases_dados[base]['dir']
        logger.info(f"Iniciando an√°lise consolidada da base: {base.upper()} (dir={dir_pre})")

        arquivos = sorted(
            glob.glob(os.path.join(dir_pre, '*.csv')) +
            glob.glob(os.path.join(dir_pre, '*.parquet'))
        )
        if not arquivos:
            logger.warning(f"Sem arquivos preprocessados para {base.upper()} em {dir_pre}")
            continue

        # Acumuladores da base
        total_registros = 0
        ti_count = 0
        outros_count = 0
        idade_soma_ti = 0.0
        idade_soma_outros = 0.0
        anos_set = set()

        colunas_essenciais = ['ano', 'idade', 'cbo_ocupacao', 'eh_ti']

        for i, arq in enumerate(arquivos, 1):
            print(f"   Arquivo {i}/{len(arquivos)}: {os.path.basename(arq)}")
            try:
                if arq.lower().endswith('.parquet'):
                    iterador = _iterar_parquet_em_batches(arq, colunas_essenciais, batch_size=200_000)
                else:
                    iterador = _ler_csv_em_chunks(arq, colunas_essenciais, chunksize=500_000)

                for chunk in iterador:
                    chunk = _processar_chunk_para_metricas(chunk, colunas_essenciais)

                    ti_mask = chunk['eh_ti'] == True
                    n_ti = int(ti_mask.sum())
                    n_total = len(chunk)
                    ti_count += n_ti
                    total_registros += n_total
                    outros_count += (n_total - n_ti)

                    if 'idade' in chunk.columns:
                        idade_soma_ti += pd.to_numeric(chunk.loc[ti_mask, 'idade'], errors='coerce').fillna(0).sum()
                        idade_soma_outros += pd.to_numeric(chunk.loc[~ti_mask, 'idade'], errors='coerce').fillna(0).sum()

                    if 'ano' in chunk.columns:
                        anos_set.update(pd.to_numeric(chunk['ano'], errors='coerce').dropna().unique())

                    del chunk
                    gc.collect()

            except Exception as e:
                logger.error(f"Erro ao processar arquivo {arq}: {e}")
                print(f"‚ùå Erro ao processar arquivo {arq}: {e}")

        idade_media_ti = (idade_soma_ti / ti_count) if ti_count > 0 else 0.0
        idade_media_outros = (idade_soma_outros / outros_count) if outros_count > 0 else 0.0

        resultados[base.upper()] = {
            'total_registros': total_registros,
            'ti_count': ti_count,
            'outros_count': outros_count,
            'idade_media_ti': idade_media_ti,
            'idade_media_outros': idade_media_outros,
            'anos': sorted(list(anos_set)),
            'dir_preprocessados': dir_pre,
        }

        # Salva relat√≥rio dessa base na pasta-m√£e (ex.: C:/TCC/dados/pnad/)
        base_dir_mae = os.path.dirname(dir_pre)  # remove "/preprocessados"
        os.makedirs(base_dir_mae, exist_ok=True)
        rel_path = os.path.join(base_dir_mae, f"relatorio_consolidado_{base}.txt")
        with open(rel_path, 'w', encoding='utf-8') as f:
            anos_txt = f"{int(min(anos_set))}-{int(max(anos_set))}" if anos_set else "N√£o dispon√≠vel"
            f.write("=== RELAT√ìRIO DE AN√ÅLISE CONSOLIDADA ===\n\n")
            f.write(f"Base: {base.upper()}\n")
            f.write(f"Total de registros processados: {total_registros:,}\n")
            f.write(f"Profissionais TI identificados: {ti_count:,}\n")
            f.write(f"Outros profissionais: {outros_count:,}\n")
            perc_ti = (ti_count / total_registros * 100) if total_registros > 0 else 0.0
            f.write(f"Percentual TI: {perc_ti:.2f}%\n")
            f.write(f"Idade m√©dia TI: {idade_media_ti:.1f}\n")
            f.write(f"Idade m√©dia Outros: {idade_media_outros:.1f}\n")
            f.write(f"Anos de cobertura: {anos_txt}\n")

        logger.info(f"‚úÖ An√°lise consolidada da base {base.upper()} conclu√≠da!")
        print(f"Relat√≥rio salvo em: {rel_path}")

    # Resumo no console
    if resultados:
        print("\n=== RESUMO CONSOLIDADO ===")
        for base, res in resultados.items():
            anos = res['anos']
            periodo = f"{int(min(anos))}-{int(max(anos))}" if anos else "ND"
            print(f"- {base}: total={res['total_registros']:,} TI={res['ti_count']:,} Outros={res['outros_count']:,} "
                  f"Idade TI={res['idade_media_ti']:.1f} Idade Outros={res['idade_media_outros']:.1f} Per√≠odo={periodo}")
    else:
        logger.error("Nenhum dado dispon√≠vel para an√°lise consolidada")

    return resultados

def main():
    configurar_log()
    if not verificar_configuracoes():
        print("\n‚ùå Verifique as configura√ß√µes antes de continuar.")
        return
    while True:
        opcao = menu_principal()
        if opcao == '0':
            print("\nEncerrando o programa...")
            break
        elif opcao == '1':
            while True:
                sub = submenu_descompactacao()
                if sub == '0':
                    break
                elif sub == '1':
                    descompactar_base('pnad', ProcessadorPNAD, 'colunas_pnad.cfg')
                elif sub == '2':
                    descompactar_base('rais', ProcessadorRAIS, 'colunas_rais.cfg')
                elif sub == '3':
                    descompactar_base('caged', ProcessadorCAGED, 'colunas_caged.cfg')
                elif sub == '4':
                    descompactar_base('pnad', ProcessadorPNAD, 'colunas_pnad.cfg')
                    descompactar_base('rais', ProcessadorRAIS, 'colunas_rais.cfg')
                    descompactar_base('caged', ProcessadorCAGED, 'colunas_caged.cfg')
                else:
                    print("Op√ß√£o inv√°lida!")
        elif opcao == '2':
            while True:
                sub = submenu_processar_bases()
                if sub == '0':
                    break
                elif sub == '1':
                    processar_base_se_preciso('pnad', ProcessadorPNAD, 'colunas_pnad.cfg')
                elif sub == '2':
                    processar_base_se_preciso('rais', ProcessadorRAIS, 'colunas_rais.cfg')
                elif sub == '3':
                    processar_base_se_preciso('caged', ProcessadorCAGED, 'colunas_caged.cfg')
                elif sub == '4':
                    processar_base_se_preciso('pnad', ProcessadorPNAD, 'colunas_pnad.cfg')
                    processar_base_se_preciso('rais', ProcessadorRAIS, 'colunas_rais.cfg')
                    processar_base_se_preciso('caged', ProcessadorCAGED, 'colunas_caged.cfg')
                else:
                    print("Op√ß√£o inv√°lida!")
        elif opcao == '3':
            while True:
                sub = submenu_relatorios()
                if sub == '0':
                    break
                elif sub == '1':
                    relatorio_base_se_preciso('pnad', ProcessadorPNAD, 'colunas_pnad.cfg', lambda: executar_analise_consolidada(['pnad']))
                elif sub == '2':
                    relatorio_base_se_preciso('rais', ProcessadorRAIS, 'colunas_rais.cfg', lambda: executar_analise_consolidada(['rais']))
                elif sub == '3':
                    relatorio_base_se_preciso('caged', ProcessadorCAGED, 'colunas_caged.cfg', lambda: executar_analise_consolidada(['caged']))
                elif sub == '4':
                    relatorio_base_se_preciso('todos', ProcessadorPNAD, 'colunas_pnad.cfg', lambda: executar_analise_consolidada(['pnad','rais','caged']))
                else:
                    print("Op√ß√£o inv√°lida!")
        else:
            print("Op√ß√£o inv√°lida!")
        input("\nPressione ENTER para continuar...")


if __name__ == '__main__':
    main()

